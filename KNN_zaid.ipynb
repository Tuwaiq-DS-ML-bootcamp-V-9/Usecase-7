{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c4ba1-85f3-4a02-ae37-c64347a01803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395e8bf-b530-4720-8d0f-26811e927d6e",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "We can now load the dataset into pandas using the read_csv() function. This converts the CSV file into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4030b0-aa7f-4bd7-8e9c-c1b7f4d202d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the csv file and convert to a Pandas dataframe\n",
    "df = pd.read_csv(\"Data/final_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ed983-0f72-43eb-8168-6fafd48c562e",
   "metadata": {},
   "source": [
    "### Viewing the dataframe\n",
    "We can get a quick sense of the size of our dataset by using the shape method. This returns a tuple with the number of rows and columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde0b69-613c-4dd1-8ebf-e4d2e570e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b5bc4-db37-4432-b666-0714afd0c4ca",
   "metadata": {},
   "source": [
    "## 1. Data Profiling:\n",
    "Data profiling is a comprehensive process of examining the data available in an existing dataset and collecting statistics and information about that data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f2081-5a2c-4908-8cbd-29519fb3cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834929aa-fd6a-4ec5-84d4-77c4b3c1a506",
   "metadata": {},
   "source": [
    "The process of profiling differs slightly for categorical and numerical variables due to their inherent differences.\n",
    "\n",
    "**The two main types of data are:**\n",
    "- Quantitative (numerical) data\n",
    "- Qualitative (categorical) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9954f5-6fbc-45b4-ad86-3e21b6e0ca2d",
   "metadata": {},
   "source": [
    "### Data Quality Checks\n",
    "Data quality checks involve the process of ensuring that the data is accurate, complete, consistent, relevant, and reliable. \n",
    "\n",
    "\n",
    "**Here are typical steps involved in checking data quality:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818c876-173d-4e56-9e7d-b4334d2def25",
   "metadata": {},
   "source": [
    "#### 1. Reliability:\n",
    "Evaluate the data's source and collection process to determine its trustworthiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034ae7b-dc1d-4cba-8f9e-bb499d021cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes it is reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0cc59-8c92-4acc-8d07-c40764e1a86b",
   "metadata": {},
   "source": [
    "#### 2. Timeliness: \n",
    "Ensure the data is up-to-date and reflective of the current situation or the period of interest for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78ae35-7226-4cb6-b8b2-a46c2ed17cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is 2021 - 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fab0fe2-6624-4615-b9d8-3c3669056bf8",
   "metadata": {},
   "source": [
    "#### 3. Consistency: \n",
    "\n",
    "Confirm that the data is consistent within the dataset and across multiple data sources. For example, the same data point should not have different values in different places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecc573-959f-4800-8ddd-a67985c68b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one data source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3d183-bba0-4b12-b963-487daab1e876",
   "metadata": {},
   "source": [
    "#### 4. Relevance: \n",
    "Assess whether the data is appropriate and applicable for the intended analysis. Data that is not relevant can skew results and lead to incorrect conclusions.\n",
    "\n",
    "**Key considerations for relevance include:**\n",
    "\n",
    "> 1. Sample Appropriateness: Confirm that your data sample aligns with your analysis objectives. For instance, utilizing data from the Northern region will not yield accurate insights for the Western region of the Kingdom.\n",
    ">\n",
    "> 2. Variable Selection: Any column will not be relevant for our analysis, we can get rid of these using the drop() method. We will set the “axis” argument to 1 since we’re dealing with columns, and set the “inplace” argument to True to make the change permanent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a934b7-a0cd-443f-8a27-f10e9a6e8647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e3a2a-bbb7-4d06-8220-c93277b70146",
   "metadata": {},
   "source": [
    "#### 5. Uniqueness: \n",
    "Check for and remove duplicate records to prevent skewed analysis results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509a7aa-58f4-4d39-8eb8-e8298a21f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c62559-3b48-48cf-a4e9-857e2e0ff416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to delete duplicates columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7724ac2-4ea2-4cf4-a9e0-e64368f87b92",
   "metadata": {},
   "source": [
    "#### 6. Completeness: \n",
    "Ensure that no critical data is missing. This might mean checking for null values or required fields that are empty.\n",
    "\n",
    "We will start by checking the dataset for missing or null values. For this, we can use the isna() method which returns a dataframe of boolean values indicating if a field is null or not. To group all missing values by column, we can include the sum() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4de54-a344-4b94-9908-9528c15c13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display number missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec31c4-0904-4b73-8cee-b7bc14ab1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to clean them "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40de63c-5a1d-49ed-a87e-c5229ee08bbe",
   "metadata": {},
   "source": [
    "#### 7. Check Accuracy:\n",
    "\n",
    "Verify that the data is correct and precise. This could involve comparing data samples with known sources or using validation rules.\n",
    "\n",
    "**The process includes:**\n",
    "1. Validating the appropriateness of data types for the dataset.\n",
    "2. Identifying outliers  using established validation  rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be5334-ae10-4abd-8097-3259fe5e72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee790ba-dcc0-45f1-b6f8-0133e913e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to clean them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b303a-5459-45f9-a28e-fcee45c21c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca9d42-add6-45db-92cf-1f6dae5f277b",
   "metadata": {},
   "source": [
    "**What is an Outlier?** \n",
    "Outlier is an row/observation that appears far away and diverges from an overall pattern in a sample.\n",
    "\n",
    "**What are the types of Outliers?**\n",
    "1. Univariate: These outliers can be found when we look at distribution of a single variable\n",
    "2. Multivariate: are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions. example (hight=100, weight=100) for a person\n",
    "\n",
    "**What causes Outliers?**\n",
    "Whenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. The method to deal with them would then depend on the reason of their occurrence.\n",
    "\n",
    "Let’s understand various types of outliers:\n",
    "\n",
    "1. Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.\n",
    "2. Measurement Error: It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty.\n",
    "3. Data Processing Error: Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\n",
    "4. Sampling error: For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\n",
    "5. Natural Outlier: When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my last assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately.\n",
    "\n",
    "\n",
    "**What is the impact of Outliers on a dataset?**\n",
    "\n",
    "\n",
    "![image.png](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/Outlier_31.png)\n",
    "\n",
    "\n",
    "\n",
    "**How to detect Outliers?**\n",
    "\n",
    "1. Most commonly used method to detect outliers is visualization (Univariate Graphical Analysis).\n",
    "\n",
    "We use 3 common visualization methods:\n",
    ">- Box-plot: A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of the box to show the range of the data. Outlier points are those past the end of the whiskers. Box plots show robust measures of location and spread as well as providing information about symmetry and outliers.\n",
    ">\n",
    ">  \n",
    ">![image.png](https://miro.medium.com/v2/resize:fit:698/format:webp/1*VK5iHA2AB28HSZwWwUbNYg.png)\n",
    ">\n",
    ">\n",
    ">- Histogram\n",
    ">- Scatter Plot: A scatter plot is a mathematical diagram using Cartesian coordinates to display values for two variables for a set of data. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. The points that are far from the population can be termed as an outlier.\n",
    ">\n",
    ">  \n",
    ">![image.png](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*Ov6aH-8yIwNoUxtMFwgx4g.png)\n",
    ">\n",
    ">\n",
    "\n",
    "2. Using statistical method (Univariate Non-Graphical analysis):\n",
    ">- Any value, which is beyond the range of -1.5 x IQR to 1.5 x IQR\n",
    " \n",
    "![image.png](https://www.whatissixsigma.net/wp-content/uploads/2015/07/Box-Plot-Diagram-to-identify-Outliers-figure-1.png)\n",
    "\n",
    ">- Use capping methods. Any value which out of range of 5th and 95th percentile can be considered as outlier\n",
    ">- Data points, three or more standard deviation away from mean are considered outlier: The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. While calculating the Z-score we re-scale and center the data and look for data points that are too far from zero. These data points which are way too far from zero will be treated as the outliers. In most of the cases, a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.\n",
    "> - Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea6194-cc01-45d8-be38-c4543eb1714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to univariate graphical analysis\n",
    "# go to lesson : data visualisation 1 - chart type section\n",
    "# then go to univariate graphical analysis\n",
    "# detect outliers using graphs varbaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecf77b-480c-4f64-9485-95be805bc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to lesson: statistics 1 then statistics 3\n",
    "# then go to univariate Non graphical analysis\n",
    "# detect outliers using numerical statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee200da8-62b0-492d-b118-f4d665a1fb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66d611-6958-4860-8522-9ada7fce40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to delete ouliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e886ec7-388c-414b-ada7-803c2fb1f2cb",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning: \n",
    "\n",
    "Preliminary findings from data profiling can lead to cleaning the data by:\n",
    "- Handling missing values\n",
    "- Correcting errors.\n",
    "- Dealing with outliers.\n",
    "\n",
    "-------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21876b48-f5ec-4970-85a9-0520d45d8841",
   "metadata": {},
   "source": [
    "### Handling missing values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890d5a2-2a65-4090-9427-f89c0f011d3f",
   "metadata": {},
   "source": [
    "**Why my data has missing values?**\n",
    "They may occur at two stages:\n",
    "1. Data Extraction: It is possible that there are problems with extraction process. Errors at data extraction stage are typically easy to find and can be corrected easily as well.\n",
    "2. Data collection: These errors occur at time of data collection and are harder to correct.\n",
    "\n",
    "**Why do we need to handle the missing data?**\n",
    "To avoid:\n",
    "- Bias the conclusions.\n",
    "- Leading the business to make wrong decisions.\n",
    "\n",
    "**Which are the methods to treat missing values ?**\n",
    "1. Deletion: we delete rows where any of the variable is missing. Simplicity is one of the major advantage of this method, but this method reduces the power of model because it reduces the sample size.\n",
    "\n",
    "2. Imputation: is a method to fill in the missing values with estimated ones. This imputation is one of the most frequently used methods.\n",
    "\n",
    "    2.1. Mean/ Mode/ Median Imputation: It consists of replacing the missing data for a given attribute by the mean or median (quantitative attribute) or mode (qualitative attribute) of all known values of that variable.\n",
    "    > It can be of two types:\n",
    "    > - Generalized Imputation: In this case, we calculate the mean or median for all non missing values of that variable then replace missing value with mean or median.\n",
    "    > - Similar case Imputation: In this case, we calculate average for each group individually of non missing values then replace the missing value based on the group.\n",
    "\n",
    "    2.2. Constant Value\n",
    "   \n",
    "    2.3. Forward Filling\n",
    "   \n",
    "    2.4. Backward Filling\n",
    "\n",
    "6. Prediction Model:  Prediction model is one of the sophisticated method for handling missing data. Here, we create a predictive model to estimate values that will substitute the missing data.  In this case, we divide our data set into two sets: One set with no missing values for the variable and another one with missing values. First data set become training data set of the model while second data set with missing values is test data set and variable with missing values is treated as target variable. Next, we create a model to predict target variable based on other attributes of the training data set and populate missing values of test data set.\n",
    "\n",
    "> There are 2 drawbacks for this approach:\n",
    "> - The model estimated values are usually more well-behaved than the true values\n",
    "> - If there are no relationships with attributes in the data set and the attribute with missing values, then the model will not be precise for estimating missing values.\n",
    "\n",
    "9. KNN Imputation: In this method of imputation, the missing values of an attribute are imputed using the given number of attributes that are most similar to the attribute whose values are missing. The similarity of two attributes is determined using a distance function. It is also known to have certain advantage & disadvantages.\n",
    "\n",
    "   > **Advantages:**\n",
    "   > - k-nearest neighbour can predict both qualitative & quantitative attributes\n",
    "   > - Creation of predictive model for each attribute with missing data is not required\n",
    "   > - Attributes with multiple missing values can be easily treated\n",
    "   > - Correlation structure of the data is taken into consideration\n",
    "\n",
    "   > **Disadvantage:**\n",
    "   > - KNN algorithm is very time-consuming in analyzing large database. It searches through all the dataset looking for the most similar instances.\n",
    "   > - Choice of k-value is very critical. Higher value of k would include attributes which are significantly different from what we need whereas lower value of k implies missing out of significant attributes.\n",
    "\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e741fb-71c1-46ad-a526-d8f0b1564dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16df21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01287962-8077-4c01-8d1d-5f8aed6cb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to 6th dimention --> Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cee10f-0af8-44e5-b595-8e965294daad",
   "metadata": {},
   "source": [
    "### Correcting errors\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06273b88-a169-42e8-81f5-5d71cb3f9c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d95da5-a3ba-473a-8243-aa177cadae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to 7th dimension Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc7dbb-6867-44cf-8f99-1b969a80be40",
   "metadata": {},
   "source": [
    "### Dealing with outliers:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88347887-4bdf-48af-9486-cb8fe80c97af",
   "metadata": {},
   "source": [
    "**How to remove Outliers?**\n",
    "Most of the ways to deal with outliers are similar to the methods of missing values like deleting rows, transforming them, binning them, treat them as a separate group, imputing values and other statistical methods. Here, we will discuss the common techniques used to deal with outliers:\n",
    "\n",
    "1. Deleting rows: We delete outlier values if it is due to data entry error, data processing error or outlier rows are very small in numbers. We can also use trimming at both ends to remove outliers.\n",
    "\n",
    "2. Imputing: Like imputation of missing values, we can also impute outliers. We can use mean, median, mode imputation methods. Before imputing values, we should analyse if it is natural outlier or artificial. If it is artificial, we can go with imputing values. We can also use statistical model to predict values of outlier rows and after that we can impute it with predicted values.\n",
    "\n",
    "3. Treat separately: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a585f0-2b9c-42fa-bf21-cacc6aa3be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"appearance\"] == 0][\"current_value\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"appearance\"] == 0) & (df[\"current_value\"] == 6500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['current_value'] ==0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(df[(df['current_value'] ==0) & (df[\"highest_value\"] == 0)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce135b2e",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2640de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ea7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.drop(columns=[\"name\",\"player\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e552d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical columns\n",
    "categorical_columns = df_model.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = df_model.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "\n",
    "# Display the categorical and numerical columns\n",
    "print(\"Categorical columns: \", categorical_columns)\n",
    "print(\"Numerical columns: \", numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5029bc",
   "metadata": {},
   "source": [
    "#### 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94a447",
   "metadata": {},
   "source": [
    "1. Feature scaling\n",
    "2. Aggregation\n",
    "3. One hot coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6550dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model=pd.get_dummies(df_model,columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bbc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 35th and 75th percentiles of the price\n",
    "p35 = df_model['current_value'].quantile(0.35)\n",
    "p75 = df_model['current_value'].quantile(0.70)\n",
    "\n",
    "# Function to categorize prices\n",
    "def categorize_price(price):\n",
    "    if price < p35:\n",
    "        return 'low_value'\n",
    "    elif price < p75:\n",
    "        return 'good_value'\n",
    "    else:\n",
    "        return 'High_value'\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df_model['current_value_category'] = df_model['current_value'].apply(categorize_price)\n",
    "\n",
    "df_model.drop('current_value', axis=1, inplace=True)\n",
    "\n",
    "# Verify the distribution of the new categories\n",
    "print(df_model['current_value_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p35,p75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92caade",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df_model['current_value_category_encoded'] = encoder.fit_transform(df_model['current_value_category'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[\"current_value_category_encoded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383c66d",
   "metadata": {},
   "source": [
    "#### 3. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation= df_model.corr(numeric_only=True)\n",
    "print(correlation['current_value_category_encoded'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correlation threshold\n",
    "threshold = 0.3\n",
    "\n",
    "# Filter the correlations\n",
    "# We use `abs()` for absolute value to consider both strong positive and negative correlations\n",
    "selected_features = correlation[abs(correlation['current_value_category_encoded']) > threshold]['current_value_category_encoded'].index\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edfc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ec4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184f3cd",
   "metadata": {},
   "source": [
    "#### 4. Prepare train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88f5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df_model.drop(['current_value_category_encoded'], axis=1)\n",
    "y = df_model['current_value_category_encoded']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# sacle the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae48d94",
   "metadata": {},
   "source": [
    "## Train the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509811ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [2,3,4,5,6,7,8,9,10,11,13,14,15],\n",
    "    }\n",
    "grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557258c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac592fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae863d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20906edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = round(df_model['current_value_category_encoded'].value_counts()[1]/df.shape[0]*100, 2)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train_scaled)\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce716f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fb374",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred,\n",
    "                                        display_labels=model.classes_,\n",
    "                                        cmap=\"Blues\",\n",
    "                                        xticks_rotation='vertical')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ae654",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "print(f\"Precision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd666aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,\n",
    "                      y_pred,\n",
    "                      labels=list(model.classes_)))\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5171a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f60e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e796f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
